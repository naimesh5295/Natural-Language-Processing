{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. Thank you to all of you in this room. I have to congratulate the other incredible nominees this year. The Revenant was the product of the tireless efforts of an unbelievable cast and crew. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience. Thank you to everybody at Fox and New Regency … my entire team. I have to thank everyone from the very onset of my career … To my parents; none of this would be possible without you. And to my friends, I love you dearly; you know who you are.\n",
    "\n",
    "And lastly, I just want to say this: Making The Revenant was about man's relationship to the natural world. A world that we collectively felt in 2015 as the hottest year in recorded history. Our production needed to move to the southern tip of this planet just to be able to find snow. Climate change is real, it is happening right now. It is the most urgent threat facing our entire species, and we need to work collectively together and stop procrastinating. We need to support leaders around the world who do not speak for the big polluters, but who speak for all of humanity, for the indigenous people of the world, for the billions and billions of underprivileged people out there who would be most affected by this. For our children’s children, and for those people out there whose voices have been drowned out by the politics of greed. I thank you all for this amazing award tonight. Let us not take this planet for granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scent tokenize\n",
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word tokenizer\n",
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "346"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming means different words with same meaning ex intelligent,intelligently = intelligen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">1.Word representations have not have any meaning\n",
    "\n",
    ">2.Takes less time\n",
    "\n",
    ">3.Use stemming when meaning if words is not important for analysis. ex.for span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer#for stemming whole paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    new_words = [stemmer.stem(word) for word in words]#newwords list creation\n",
    "    sentences[i] = ' '.join(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thank you all so veri much .',\n",
       " 'thank you to the academi .',\n",
       " 'thank you to all of you in thi room .',\n",
       " 'I have to congratul the other incred nomine thi year .',\n",
       " 'the reven wa the product of the tireless effort of an unbeliev cast and crew .',\n",
       " 'first off , to my brother in thi endeavor , mr. tom hardi .',\n",
       " 'tom , your talent on screen can onli be surpass by your friendship off screen … thank you for creat a transcend cinemat experi .',\n",
       " 'thank you to everybodi at fox and new regenc … my entir team .',\n",
       " 'I have to thank everyon from the veri onset of my career … To my parent ; none of thi would be possibl without you .',\n",
       " 'and to my friend , I love you dearli ; you know who you are .',\n",
       " \"and lastli , I just want to say thi : make the reven wa about man 's relationship to the natur world .\",\n",
       " 'A world that we collect felt in 2015 as the hottest year in record histori .',\n",
       " 'our product need to move to the southern tip of thi planet just to be abl to find snow .',\n",
       " 'climat chang is real , it is happen right now .',\n",
       " 'It is the most urgent threat face our entir speci , and we need to work collect togeth and stop procrastin .',\n",
       " 'We need to support leader around the world who do not speak for the big pollut , but who speak for all of human , for the indigen peopl of the world , for the billion and billion of underprivileg peopl out there who would be most affect by thi .',\n",
       " 'for our children ’ s children , and for those peopl out there whose voic have been drown out by the polit of greed .',\n",
       " 'I thank you all for thi amaz award tonight .',\n",
       " 'let us not take thi planet for grant .',\n",
       " 'I do not take tonight for grant .',\n",
       " 'thank you so veri much .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization means converting different word to root word\n",
    "\n",
    ">1.Word representations have meaning\n",
    "\n",
    ">2.Takes more time than stemminmg\n",
    "\n",
    ">3.Use Lemmatization when meaning of words is importtant for analysys. ex-question answering app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmataizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [lemmataizer.lemmatize(word) for word in words]\n",
    "    sentences[i] = ' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank you all so very much .',\n",
       " 'Thank you to the Academy .',\n",
       " 'Thank you to all of you in this room .',\n",
       " 'I have to congratulate the other incredible nominee this year .',\n",
       " 'The Revenant wa the product of the tireless effort of an unbelievable cast and crew .',\n",
       " 'First off , to my brother in this endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , your talent on screen can only be surpassed by your friendship off screen … thank you for creating a transcendent cinematic experience .',\n",
       " 'Thank you to everybody at Fox and New Regency … my entire team .',\n",
       " 'I have to thank everyone from the very onset of my career … To my parent ; none of this would be possible without you .',\n",
       " 'And to my friend , I love you dearly ; you know who you are .',\n",
       " \"And lastly , I just want to say this : Making The Revenant wa about man 's relationship to the natural world .\",\n",
       " 'A world that we collectively felt in 2015 a the hottest year in recorded history .',\n",
       " 'Our production needed to move to the southern tip of this planet just to be able to find snow .',\n",
       " 'Climate change is real , it is happening right now .',\n",
       " 'It is the most urgent threat facing our entire specie , and we need to work collectively together and stop procrastinating .',\n",
       " 'We need to support leader around the world who do not speak for the big polluter , but who speak for all of humanity , for the indigenous people of the world , for the billion and billion of underprivileged people out there who would be most affected by this .',\n",
       " 'For our child ’ s child , and for those people out there whose voice have been drowned out by the politics of greed .',\n",
       " 'I thank you all for this amazing award tonight .',\n",
       " 'Let u not take this planet for granted .',\n",
       " 'I do not take tonight for granted .',\n",
       " 'Thank you so very much .']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Words that dont express any meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/naimeshpatel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    newwords = [word for word in words if word not in stopwords.words('english')]\n",
    "    sentences[i] = ' '.join(newwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank much .',\n",
       " 'Thank Academy .',\n",
       " 'Thank room .',\n",
       " 'I congratulate incredible nominees year .',\n",
       " 'The Revenant product tireless efforts unbelievable cast crew .',\n",
       " 'First , brother endeavor , Mr. Tom Hardy .',\n",
       " 'Tom , talent screen surpassed friendship screen … thank creating transcendent cinematic experience .',\n",
       " 'Thank everybody Fox New Regency … entire team .',\n",
       " 'I thank everyone onset career … To parents ; none would possible without .',\n",
       " 'And friends , I love dearly ; know .',\n",
       " \"And lastly , I want say : Making The Revenant man 's relationship natural world .\",\n",
       " 'A world collectively felt 2015 hottest year recorded history .',\n",
       " 'Our production needed move southern tip planet able find snow .',\n",
       " 'Climate change real , happening right .',\n",
       " 'It urgent threat facing entire species , need work collectively together stop procrastinating .',\n",
       " 'We need support leaders around world speak big polluters , speak humanity , indigenous people world , billions billions underprivileged people would affected .',\n",
       " 'For children ’ children , people whose voices drowned politics greed .',\n",
       " 'I thank amazing award tonight .',\n",
       " 'Let us take planet granted .',\n",
       " 'I take tonight granted .',\n",
       " 'Thank much .']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Assigns parts of speech to each word (and other token), such as noun, verb, adjective, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('Academy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('room', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('congratulate', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('other', 'JJ'),\n",
       " ('incredible', 'JJ'),\n",
       " ('nominees', 'NNS'),\n",
       " ('this', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('product', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('tireless', 'NN'),\n",
       " ('efforts', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('unbelievable', 'JJ'),\n",
       " ('cast', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('crew', 'NN'),\n",
       " ('.', '.'),\n",
       " ('First', 'NNP'),\n",
       " ('off', 'RB'),\n",
       " (',', ','),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('brother', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('endeavor', 'NN'),\n",
       " (',', ','),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Tom', 'NNP'),\n",
       " ('Hardy', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Tom', 'NNP'),\n",
       " (',', ','),\n",
       " ('your', 'PRP$'),\n",
       " ('talent', 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('screen', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('only', 'RB'),\n",
       " ('be', 'VB'),\n",
       " ('surpassed', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('your', 'PRP$'),\n",
       " ('friendship', 'NN'),\n",
       " ('off', 'IN'),\n",
       " ('screen', 'JJ'),\n",
       " ('…', 'NNP'),\n",
       " ('thank', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('for', 'IN'),\n",
       " ('creating', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('transcendent', 'JJ'),\n",
       " ('cinematic', 'JJ'),\n",
       " ('experience', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('everybody', 'VB'),\n",
       " ('at', 'IN'),\n",
       " ('Fox', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('New', 'NNP'),\n",
       " ('Regency', 'NNP'),\n",
       " ('…', 'NNP'),\n",
       " ('my', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('team', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('thank', 'VB'),\n",
       " ('everyone', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('onset', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('career', 'NN'),\n",
       " ('…', 'NN'),\n",
       " ('To', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('parents', 'NNS'),\n",
       " (';', ':'),\n",
       " ('none', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('possible', 'JJ'),\n",
       " ('without', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('to', 'TO'),\n",
       " ('my', 'PRP$'),\n",
       " ('friends', 'NNS'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('love', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('dearly', 'RB'),\n",
       " (';', ':'),\n",
       " ('you', 'PRP'),\n",
       " ('know', 'VBP'),\n",
       " ('who', 'WP'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('.', '.'),\n",
       " ('And', 'CC'),\n",
       " ('lastly', 'RB'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('just', 'RB'),\n",
       " ('want', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('say', 'VB'),\n",
       " ('this', 'DT'),\n",
       " (':', ':'),\n",
       " ('Making', 'VBG'),\n",
       " ('The', 'DT'),\n",
       " ('Revenant', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('man', 'NN'),\n",
       " (\"'s\", 'POS'),\n",
       " ('relationship', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('natural', 'JJ'),\n",
       " ('world', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('collectively', 'RB'),\n",
       " ('felt', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('2015', 'CD'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('hottest', 'JJS'),\n",
       " ('year', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('recorded', 'JJ'),\n",
       " ('history', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Our', 'PRP$'),\n",
       " ('production', 'NN'),\n",
       " ('needed', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('move', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('southern', 'JJ'),\n",
       " ('tip', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('just', 'RB'),\n",
       " ('to', 'TO'),\n",
       " ('be', 'VB'),\n",
       " ('able', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('find', 'VB'),\n",
       " ('snow', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('Climate', 'NNP'),\n",
       " ('change', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('real', 'JJ'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('happening', 'VBG'),\n",
       " ('right', 'RB'),\n",
       " ('now', 'RB'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('most', 'RBS'),\n",
       " ('urgent', 'JJ'),\n",
       " ('threat', 'NN'),\n",
       " ('facing', 'VBG'),\n",
       " ('our', 'PRP$'),\n",
       " ('entire', 'JJ'),\n",
       " ('species', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('collectively', 'RB'),\n",
       " ('together', 'RB'),\n",
       " ('and', 'CC'),\n",
       " ('stop', 'VB'),\n",
       " ('procrastinating', 'NN'),\n",
       " ('.', '.'),\n",
       " ('We', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('support', 'VB'),\n",
       " ('leaders', 'NNS'),\n",
       " ('around', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('speak', 'VB'),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('polluters', 'NNS'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('who', 'WP'),\n",
       " ('speak', 'VBP'),\n",
       " ('for', 'IN'),\n",
       " ('all', 'DT'),\n",
       " ('of', 'IN'),\n",
       " ('humanity', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('indigenous', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('world', 'NN'),\n",
       " (',', ','),\n",
       " ('for', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('billions', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('billions', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('underprivileged', 'JJ'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'IN'),\n",
       " ('there', 'EX'),\n",
       " ('who', 'WP'),\n",
       " ('would', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('most', 'RBS'),\n",
       " ('affected', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('.', '.'),\n",
       " ('For', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('children', 'NNS'),\n",
       " ('’', 'VBP'),\n",
       " ('s', 'JJ'),\n",
       " ('children', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('for', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('out', 'RP'),\n",
       " ('there', 'RB'),\n",
       " ('whose', 'WP$'),\n",
       " ('voices', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('drowned', 'VBN'),\n",
       " ('out', 'RP'),\n",
       " ('by', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('politics', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('greed', 'NN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('thank', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('all', 'DT'),\n",
       " ('for', 'IN'),\n",
       " ('this', 'DT'),\n",
       " ('amazing', 'JJ'),\n",
       " ('award', 'NN'),\n",
       " ('tonight', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Let', 'VB'),\n",
       " ('us', 'PRP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('this', 'DT'),\n",
       " ('planet', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('take', 'VB'),\n",
       " ('tonight', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('granted', 'VBN'),\n",
       " ('.', '.'),\n",
       " ('Thank', 'NNP'),\n",
       " ('you', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('very', 'RB'),\n",
       " ('much', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tags=[]\n",
    "for i in tagged_words:\n",
    "    word_tags.append(i[0]+\"_\"+i[1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'all_DT',\n",
       " 'so_RB',\n",
       " 'very_RB',\n",
       " 'much_JJ',\n",
       " '._.',\n",
       " 'Thank_VB',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'Academy_NNP',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'all_DT',\n",
       " 'of_IN',\n",
       " 'you_PRP',\n",
       " 'in_IN',\n",
       " 'this_DT',\n",
       " 'room_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'have_VBP',\n",
       " 'to_TO',\n",
       " 'congratulate_VB',\n",
       " 'the_DT',\n",
       " 'other_JJ',\n",
       " 'incredible_JJ',\n",
       " 'nominees_NNS',\n",
       " 'this_DT',\n",
       " 'year_NN',\n",
       " '._.',\n",
       " 'The_DT',\n",
       " 'Revenant_NNP',\n",
       " 'was_VBD',\n",
       " 'the_DT',\n",
       " 'product_NN',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'tireless_NN',\n",
       " 'efforts_NNS',\n",
       " 'of_IN',\n",
       " 'an_DT',\n",
       " 'unbelievable_JJ',\n",
       " 'cast_NN',\n",
       " 'and_CC',\n",
       " 'crew_NN',\n",
       " '._.',\n",
       " 'First_NNP',\n",
       " 'off_RB',\n",
       " ',_,',\n",
       " 'to_TO',\n",
       " 'my_PRP$',\n",
       " 'brother_NN',\n",
       " 'in_IN',\n",
       " 'this_DT',\n",
       " 'endeavor_NN',\n",
       " ',_,',\n",
       " 'Mr._NNP',\n",
       " 'Tom_NNP',\n",
       " 'Hardy_NNP',\n",
       " '._.',\n",
       " 'Tom_NNP',\n",
       " ',_,',\n",
       " 'your_PRP$',\n",
       " 'talent_NN',\n",
       " 'on_IN',\n",
       " 'screen_NN',\n",
       " 'can_MD',\n",
       " 'only_RB',\n",
       " 'be_VB',\n",
       " 'surpassed_VBN',\n",
       " 'by_IN',\n",
       " 'your_PRP$',\n",
       " 'friendship_NN',\n",
       " 'off_IN',\n",
       " 'screen_JJ',\n",
       " '…_NNP',\n",
       " 'thank_NN',\n",
       " 'you_PRP',\n",
       " 'for_IN',\n",
       " 'creating_VBG',\n",
       " 'a_DT',\n",
       " 'transcendent_JJ',\n",
       " 'cinematic_JJ',\n",
       " 'experience_NN',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'to_TO',\n",
       " 'everybody_VB',\n",
       " 'at_IN',\n",
       " 'Fox_NNP',\n",
       " 'and_CC',\n",
       " 'New_NNP',\n",
       " 'Regency_NNP',\n",
       " '…_NNP',\n",
       " 'my_PRP$',\n",
       " 'entire_JJ',\n",
       " 'team_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'have_VBP',\n",
       " 'to_TO',\n",
       " 'thank_VB',\n",
       " 'everyone_NN',\n",
       " 'from_IN',\n",
       " 'the_DT',\n",
       " 'very_RB',\n",
       " 'onset_NN',\n",
       " 'of_IN',\n",
       " 'my_PRP$',\n",
       " 'career_NN',\n",
       " '…_NN',\n",
       " 'To_TO',\n",
       " 'my_PRP$',\n",
       " 'parents_NNS',\n",
       " ';_:',\n",
       " 'none_NN',\n",
       " 'of_IN',\n",
       " 'this_DT',\n",
       " 'would_MD',\n",
       " 'be_VB',\n",
       " 'possible_JJ',\n",
       " 'without_IN',\n",
       " 'you_PRP',\n",
       " '._.',\n",
       " 'And_CC',\n",
       " 'to_TO',\n",
       " 'my_PRP$',\n",
       " 'friends_NNS',\n",
       " ',_,',\n",
       " 'I_PRP',\n",
       " 'love_VBP',\n",
       " 'you_PRP',\n",
       " 'dearly_RB',\n",
       " ';_:',\n",
       " 'you_PRP',\n",
       " 'know_VBP',\n",
       " 'who_WP',\n",
       " 'you_PRP',\n",
       " 'are_VBP',\n",
       " '._.',\n",
       " 'And_CC',\n",
       " 'lastly_RB',\n",
       " ',_,',\n",
       " 'I_PRP',\n",
       " 'just_RB',\n",
       " 'want_VBP',\n",
       " 'to_TO',\n",
       " 'say_VB',\n",
       " 'this_DT',\n",
       " ':_:',\n",
       " 'Making_VBG',\n",
       " 'The_DT',\n",
       " 'Revenant_NNP',\n",
       " 'was_VBD',\n",
       " 'about_IN',\n",
       " 'man_NN',\n",
       " \"'s_POS\",\n",
       " 'relationship_NN',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'natural_JJ',\n",
       " 'world_NN',\n",
       " '._.',\n",
       " 'A_DT',\n",
       " 'world_NN',\n",
       " 'that_IN',\n",
       " 'we_PRP',\n",
       " 'collectively_RB',\n",
       " 'felt_VBD',\n",
       " 'in_IN',\n",
       " '2015_CD',\n",
       " 'as_IN',\n",
       " 'the_DT',\n",
       " 'hottest_JJS',\n",
       " 'year_NN',\n",
       " 'in_IN',\n",
       " 'recorded_JJ',\n",
       " 'history_NN',\n",
       " '._.',\n",
       " 'Our_PRP$',\n",
       " 'production_NN',\n",
       " 'needed_VBN',\n",
       " 'to_TO',\n",
       " 'move_VB',\n",
       " 'to_TO',\n",
       " 'the_DT',\n",
       " 'southern_JJ',\n",
       " 'tip_NN',\n",
       " 'of_IN',\n",
       " 'this_DT',\n",
       " 'planet_NN',\n",
       " 'just_RB',\n",
       " 'to_TO',\n",
       " 'be_VB',\n",
       " 'able_JJ',\n",
       " 'to_TO',\n",
       " 'find_VB',\n",
       " 'snow_JJ',\n",
       " '._.',\n",
       " 'Climate_NNP',\n",
       " 'change_NN',\n",
       " 'is_VBZ',\n",
       " 'real_JJ',\n",
       " ',_,',\n",
       " 'it_PRP',\n",
       " 'is_VBZ',\n",
       " 'happening_VBG',\n",
       " 'right_RB',\n",
       " 'now_RB',\n",
       " '._.',\n",
       " 'It_PRP',\n",
       " 'is_VBZ',\n",
       " 'the_DT',\n",
       " 'most_RBS',\n",
       " 'urgent_JJ',\n",
       " 'threat_NN',\n",
       " 'facing_VBG',\n",
       " 'our_PRP$',\n",
       " 'entire_JJ',\n",
       " 'species_NNS',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'we_PRP',\n",
       " 'need_VBP',\n",
       " 'to_TO',\n",
       " 'work_VB',\n",
       " 'collectively_RB',\n",
       " 'together_RB',\n",
       " 'and_CC',\n",
       " 'stop_VB',\n",
       " 'procrastinating_NN',\n",
       " '._.',\n",
       " 'We_PRP',\n",
       " 'need_VBP',\n",
       " 'to_TO',\n",
       " 'support_VB',\n",
       " 'leaders_NNS',\n",
       " 'around_IN',\n",
       " 'the_DT',\n",
       " 'world_NN',\n",
       " 'who_WP',\n",
       " 'do_VBP',\n",
       " 'not_RB',\n",
       " 'speak_VB',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'big_JJ',\n",
       " 'polluters_NNS',\n",
       " ',_,',\n",
       " 'but_CC',\n",
       " 'who_WP',\n",
       " 'speak_VBP',\n",
       " 'for_IN',\n",
       " 'all_DT',\n",
       " 'of_IN',\n",
       " 'humanity_NN',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'indigenous_JJ',\n",
       " 'people_NNS',\n",
       " 'of_IN',\n",
       " 'the_DT',\n",
       " 'world_NN',\n",
       " ',_,',\n",
       " 'for_IN',\n",
       " 'the_DT',\n",
       " 'billions_NNS',\n",
       " 'and_CC',\n",
       " 'billions_NNS',\n",
       " 'of_IN',\n",
       " 'underprivileged_JJ',\n",
       " 'people_NNS',\n",
       " 'out_IN',\n",
       " 'there_EX',\n",
       " 'who_WP',\n",
       " 'would_MD',\n",
       " 'be_VB',\n",
       " 'most_RBS',\n",
       " 'affected_VBN',\n",
       " 'by_IN',\n",
       " 'this_DT',\n",
       " '._.',\n",
       " 'For_IN',\n",
       " 'our_PRP$',\n",
       " 'children_NNS',\n",
       " '’_VBP',\n",
       " 's_JJ',\n",
       " 'children_NNS',\n",
       " ',_,',\n",
       " 'and_CC',\n",
       " 'for_IN',\n",
       " 'those_DT',\n",
       " 'people_NNS',\n",
       " 'out_RP',\n",
       " 'there_RB',\n",
       " 'whose_WP$',\n",
       " 'voices_NNS',\n",
       " 'have_VBP',\n",
       " 'been_VBN',\n",
       " 'drowned_VBN',\n",
       " 'out_RP',\n",
       " 'by_IN',\n",
       " 'the_DT',\n",
       " 'politics_NNS',\n",
       " 'of_IN',\n",
       " 'greed_NN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'thank_VBP',\n",
       " 'you_PRP',\n",
       " 'all_DT',\n",
       " 'for_IN',\n",
       " 'this_DT',\n",
       " 'amazing_JJ',\n",
       " 'award_NN',\n",
       " 'tonight_NN',\n",
       " '._.',\n",
       " 'Let_VB',\n",
       " 'us_PRP',\n",
       " 'not_RB',\n",
       " 'take_VB',\n",
       " 'this_DT',\n",
       " 'planet_NN',\n",
       " 'for_IN',\n",
       " 'granted_VBN',\n",
       " '._.',\n",
       " 'I_PRP',\n",
       " 'do_VBP',\n",
       " 'not_RB',\n",
       " 'take_VB',\n",
       " 'tonight_NN',\n",
       " 'for_IN',\n",
       " 'granted_VBN',\n",
       " '._.',\n",
       " 'Thank_NNP',\n",
       " 'you_PRP',\n",
       " 'so_RB',\n",
       " 'very_RB',\n",
       " 'much_JJ',\n",
       " '._.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tagged_paragraph = \" \".join(word_tags)\n",
    "tagged_paragraph = \" |  \".join(word_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Thank_NNP |  you_PRP |  all_DT |  so_RB |  very_RB |  much_JJ |  ._. |  Thank_VB |  you_PRP |  to_TO |  the_DT |  Academy_NNP |  ._. |  Thank_NNP |  you_PRP |  to_TO |  all_DT |  of_IN |  you_PRP |  in_IN |  this_DT |  room_NN |  ._. |  I_PRP |  have_VBP |  to_TO |  congratulate_VB |  the_DT |  other_JJ |  incredible_JJ |  nominees_NNS |  this_DT |  year_NN |  ._. |  The_DT |  Revenant_NNP |  was_VBD |  the_DT |  product_NN |  of_IN |  the_DT |  tireless_NN |  efforts_NNS |  of_IN |  an_DT |  unbelievable_JJ |  cast_NN |  and_CC |  crew_NN |  ._. |  First_NNP |  off_RB |  ,_, |  to_TO |  my_PRP$ |  brother_NN |  in_IN |  this_DT |  endeavor_NN |  ,_, |  Mr._NNP |  Tom_NNP |  Hardy_NNP |  ._. |  Tom_NNP |  ,_, |  your_PRP$ |  talent_NN |  on_IN |  screen_NN |  can_MD |  only_RB |  be_VB |  surpassed_VBN |  by_IN |  your_PRP$ |  friendship_NN |  off_IN |  screen_JJ |  …_NNP |  thank_NN |  you_PRP |  for_IN |  creating_VBG |  a_DT |  transcendent_JJ |  cinematic_JJ |  experience_NN |  ._. |  Thank_NNP |  you_PRP |  to_TO |  everybody_VB |  at_IN |  Fox_NNP |  and_CC |  New_NNP |  Regency_NNP |  …_NNP |  my_PRP$ |  entire_JJ |  team_NN |  ._. |  I_PRP |  have_VBP |  to_TO |  thank_VB |  everyone_NN |  from_IN |  the_DT |  very_RB |  onset_NN |  of_IN |  my_PRP$ |  career_NN |  …_NN |  To_TO |  my_PRP$ |  parents_NNS |  ;_: |  none_NN |  of_IN |  this_DT |  would_MD |  be_VB |  possible_JJ |  without_IN |  you_PRP |  ._. |  And_CC |  to_TO |  my_PRP$ |  friends_NNS |  ,_, |  I_PRP |  love_VBP |  you_PRP |  dearly_RB |  ;_: |  you_PRP |  know_VBP |  who_WP |  you_PRP |  are_VBP |  ._. |  And_CC |  lastly_RB |  ,_, |  I_PRP |  just_RB |  want_VBP |  to_TO |  say_VB |  this_DT |  :_: |  Making_VBG |  The_DT |  Revenant_NNP |  was_VBD |  about_IN |  man_NN |  's_POS |  relationship_NN |  to_TO |  the_DT |  natural_JJ |  world_NN |  ._. |  A_DT |  world_NN |  that_IN |  we_PRP |  collectively_RB |  felt_VBD |  in_IN |  2015_CD |  as_IN |  the_DT |  hottest_JJS |  year_NN |  in_IN |  recorded_JJ |  history_NN |  ._. |  Our_PRP$ |  production_NN |  needed_VBN |  to_TO |  move_VB |  to_TO |  the_DT |  southern_JJ |  tip_NN |  of_IN |  this_DT |  planet_NN |  just_RB |  to_TO |  be_VB |  able_JJ |  to_TO |  find_VB |  snow_JJ |  ._. |  Climate_NNP |  change_NN |  is_VBZ |  real_JJ |  ,_, |  it_PRP |  is_VBZ |  happening_VBG |  right_RB |  now_RB |  ._. |  It_PRP |  is_VBZ |  the_DT |  most_RBS |  urgent_JJ |  threat_NN |  facing_VBG |  our_PRP$ |  entire_JJ |  species_NNS |  ,_, |  and_CC |  we_PRP |  need_VBP |  to_TO |  work_VB |  collectively_RB |  together_RB |  and_CC |  stop_VB |  procrastinating_NN |  ._. |  We_PRP |  need_VBP |  to_TO |  support_VB |  leaders_NNS |  around_IN |  the_DT |  world_NN |  who_WP |  do_VBP |  not_RB |  speak_VB |  for_IN |  the_DT |  big_JJ |  polluters_NNS |  ,_, |  but_CC |  who_WP |  speak_VBP |  for_IN |  all_DT |  of_IN |  humanity_NN |  ,_, |  for_IN |  the_DT |  indigenous_JJ |  people_NNS |  of_IN |  the_DT |  world_NN |  ,_, |  for_IN |  the_DT |  billions_NNS |  and_CC |  billions_NNS |  of_IN |  underprivileged_JJ |  people_NNS |  out_IN |  there_EX |  who_WP |  would_MD |  be_VB |  most_RBS |  affected_VBN |  by_IN |  this_DT |  ._. |  For_IN |  our_PRP$ |  children_NNS |  ’_VBP |  s_JJ |  children_NNS |  ,_, |  and_CC |  for_IN |  those_DT |  people_NNS |  out_RP |  there_RB |  whose_WP$ |  voices_NNS |  have_VBP |  been_VBN |  drowned_VBN |  out_RP |  by_IN |  the_DT |  politics_NNS |  of_IN |  greed_NN |  ._. |  I_PRP |  thank_VBP |  you_PRP |  all_DT |  for_IN |  this_DT |  amazing_JJ |  award_NN |  tonight_NN |  ._. |  Let_VB |  us_PRP |  not_RB |  take_VB |  this_DT |  planet_NN |  for_IN |  granted_VBN |  ._. |  I_PRP |  do_VBP |  not_RB |  take_VB |  tonight_NN |  for_IN |  granted_VBN |  ._. |  Thank_NNP |  you_PRP |  so_RB |  very_RB |  much_JJ |  ._.\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity Recoginition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First step towards information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"The Taj Mahal was built by Emperor Shah Jahan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Taj', 'Mahal', 'was', 'built', 'by', 'Emperor', 'Shah', 'Jahan']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Taj', 'NNP'),\n",
       " ('Mahal', 'NNP'),\n",
       " ('was', 'VBD'),\n",
       " ('built', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Emperor', 'NNP'),\n",
       " ('Shah', 'NNP'),\n",
       " ('Jahan', 'NNP')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "namedEnt = nltk.ne_chunk(tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open gui with tree output\n",
    "namedEnt.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
